---
title: "validateHOT - An Introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{validateHOT - An Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\\VignetteDepends{dplyr}
  %\\VignetteDepends{magrittr}
bibliography: references.bib
---

## What is validateHOT?

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

validateHOT is a package designed for conjoint and MaxDiff users. It offers 
functions to validate validation tasks, perform market simulations, and convert
raw utility estimates into more interpretable scores. All three components are
key functions for preference measurement techniques such as choice-based
conjoint (CBC), adaptive choice-based conjoint (ACBC), or maximum difference
scaling (MaxDiff).

This vignette should present you ...

-   ... how to create utility scores for a validation task of a CBC, ACBC, or MaxDiff based on the **raw logit utilities** from a hierarchical Bayes estimation performed, e.g., using Sawtooth Software [@sawtooth2024]

-   ... how to measure validation metrics for a validation task:

    -   Hit Rate (`hirate()`)
    -   Kullback-Leibler-Divergence (`kl()`)
    -   Mean Absolute Error (`mae()`)
    -   Mean Hit Probability (`mhp()`)
    -   Median Absolute Error (`medae()`)
    -   Root Mean Square Error (`rmse()`)

-   ... how to measure metrics of the confusion matrix:

    -   Accuracy (`accuracy()`)
    -   F1-score (`f1()`)
    -   Precision (`precision()`)
    -   Recall (`recall()`)
    -   Specificity (`specificity()`)

-   ... how to use the validateHOT package for simulation purposes:

    -   Determine the perfect product assortment with the highest reach and frequency (`turf()`).
    -   Determine the optimal order for the assortment implication to get the highest reach (and frequency; `turf_ladder()`).
    -   Calculate the frequency of products bought on average of a specified assortment (`freqassort()`).
    -   Test the reach (`reach()`) and frequency (`freqassort()`) of a specific assortment.
    -   Perform market simulations (`marksim()`).

-   ... and finally, how to use the validateHOT package to convert raw utilities into scores that are easier to interpret:

    -   Attribute importance for CBC and ACBC (`att_imp()`).
    -   Probability scores for (unanchored) MaxDiff (`prob_scores()`).
    -   Zero-centered diffs for CBC and ACBC (`zc_diffs()`).
    -   Zero-anchored interval scores for (anchored) MaxDiff (`zero_anchored()`).

## Installation

You can install validateHOT by using the remotes package [@remotes].

```{r, eval = FALSE}
install.packages("remotes")
remotes::install_github(
  repo = "JoshSchramm94/validateHOT",
  dependencies = TRUE,
  build_vignettes = TRUE
)
```

After installation, load the validateHOT package.

```{r setup}
library(validateHOT)
```

Additionally, load the dplyr [@dplyr] package and the magrittr [@magrittr] packages.

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(magrittr)
```

## Getting started

### `create_hot()`: creating a validation task in R

First, we create the validation task, which involves calculating the utilities for each alternative presented in the validation task or in a market scenario. While this is quite easy for a MaxDiff, it can become more challenging using a CBC or an ACBC, since you might have to interpolate values based on how they were coded in the hierarchical Bayes (HB) estimation.

#### MaxDiff

Let us begin with a MaxDiff example. We use the `maxdiff` data set that comes with the package [@schramm2024] and store the utilities in a new `data.frame` object (`hot_mxd`). In this example, we have a validation task with seven alternatives plus a no-buy alternative, thus a total of eight alternatives a participant can choose from. To create the validation task, we use the `create_hot()` function and define the `data.frame` object in the `data` argument. Next, we specify the arguments `id` (participant identifier) and `choice` (the actual choice in the validation task). To define the seven alternatives in the validation task, we use the `prod.levels` argument and specify the alternatives' column indexes in a `list`. If additional variables from the original data set need to be retained, we could specify them in the `varskeep` argument.

**Note: Please be aware that you can also use the column names instead of the column indexes. However, if you include linear-coded or piecewise-coded variables, you have to specify the column indexes in `prod.levels`!**

```{r, eval = T}
# load the data
data(maxdiff)

hot_mxd <- create_hot(
  data = maxdiff,
  id = "id",
  none = "none",
  prod.levels = list(2, 9, 10, 14, 15, 16, 17),
  method = "maxdiff",
  varskeep = "group",
  choice = "hot"
)
```

If you have a forced-choice validation task, you can leave the `none` argument empty. Finally, if you want to create a market scenario instead of a validation task, you can leave the `choice` argument empty.

### Choice-Based Conjoint

#### Part-Worth utilities only

In case, you conduct a CBC (`method = "cbc"`) and all attributes are coded as part-worth, the following example demonstrates how to create the total utilities for your alternatives. For this example, we use the `cbc` data set that is part of the validateHOT package [@sablotny-wackershauser2024]. This data set includes a free-choice task with six alternatives and a no-buy alternative, respectively.

When using *CBC* or *ACBC*, we need to specify the `coding` argument.

-   `0` stands for part-worth coding,

-   `1` for linear coding, and

-   `2` for piecewise-coding.

Since all attributes in this example are part-worth coded, we set all values in `coding` to `0` (`coding = c(rep(0, times = 9))`). If you want to treat a part-worth coded variable as continuous (i.e., interpolate values) in a validation task or a market scenario, this attribute needs the code `2` (i.e., piecewise-coded).

Finally, we define the attribute levels for each alternative in the `prod.levels` argument. Similar to the MaxDiff example, since all attributes are part-worth coded, we can specify either the column indexes or column names.

```{r, eval = T}
# load the data
data(cbc)

# create the validation task
hot_cbc <- create_hot(
  data = cbc,
  id = "id",
  none = "none",
  prod.levels = list(
    c(3, 6, 10, 13, 16, 20, 24, 32, 35),
    c(3, 5, 10, 14, 16, 18, 22, 27, 35),
    c(4, 6, 9, 14, 15, 20, 25, 30, 36),
    c(4, 5, 10, 11, 16, 19, 26, 32, 34),
    c(2, 6, 8, 14, 16, 17, 26, 31, 36),
    c(2, 5, 7, 12, 16, 20, 26, 29, 33)
  ),
  coding = c(rep(0, times = 9)),
  method = "cbc",
  choice = "hot"
)
```

#### Linear-coded attribute(s)

In some cases, instead of having all attributes coded as part-worth, you may include one (or more) linear-coded attributes. The following example demonstrates how to handle this scenario. Using the same settings as in the part-worth example above, you now need to include a `1` in the `coding` argument to indicate linear coding (`coding = c(rep(0, times = 8), 1)`). This tells the `create_hot()` function that the first eight attributes are part-worth coded, while the last attribute is linear-coded. For the linear coded attribute (here price), you specify a value that this alternative should have. For example, the first alternative in our validation task is assumed to have a price of `248.55`, the second a price of `237.39`, etc.

Since `create_hot()` interpolates these values, you need to define the values used for the attribute in the hierarchical Bayes estimation. The values need to be the same as specified for the hierarchical Bayes estimation (see details below). You specify these values in a list format in the `interpolate.levels` argument. In this example of `cbc_linear`, there are six values ranging from 175.99 to 350.99. Please make sure to include **all** levels of the linear-coded attribute. In addition, you have to specify the column name or column index of the linear-coded attribute in the `lin.p` argument.

Finally, in this example, we also want to retain certain variables from the original data frame that should be included in the new `data.frame` object (see `varskeep` argument).

> [**Important**]{.underline}:
>
> 1.  If the value to be interpolated is outside the range specified in `interpolate.levels`, `create_hot()` will display a warning and apply extrapolation.
> 2.  Extrapolation is only possible for linear-coded variables not piecewise-coded variables.

```{r}
# load the data
data(cbc_linear)

# create the validation task
hot_cbc_linear <- create_hot(
  data = cbc_linear,
  id = "id",
  none = "none",
  prod.levels = list(
    c(3, 6, 10, 13, 16, 20, 24, 32, 248.55),
    c(3, 5, 10, 14, 16, 18, 22, 27, 237.39),
    c(4, 6, 9, 14, 15, 20, 25, 30, 273.15),
    c(4, 5, 10, 11, 16, 19, 26, 32, 213.55),
    c(2, 6, 8, 14, 16, 17, 26, 31, 266.10),
    c(2, 5, 7, 12, 16, 20, 26, 29, 184.50)
  ),
  coding = c(rep(0, times = 8), 1),
  lin.p = 33,
  interpolate.levels = list(c(seq(from = 175.99, to = 350.99, by = 35))),
  method = "cbc",
  choice = "hot"
)
```

#### Adaptive Choice-Based Conjoint (ACBC) with linear-coded price

Imagine you are estimating your ACBC results with a linear-coded price. When using Sawtooth Software [@sawtooth2024] for estimation, you get two coefficients, one for the lower and one for the upper coefficient. They sum up to `0`. If you estimate the hierarchical Bayes model using another package, you may receive a single coefficient instead. We provide an example for the latter below.

We use the same validation task as before (i.e., six alternatives plus a no-buy alternative).

After defining each alternative in the validation task, let us look at the new arguments in `create_hot()` function. For `interpolate.levels`, we define the lower bound of our price (`121.95`) and the upper bound of the price (`507.95`). `piece.p` specifies the position of the lower and the upper bound of the value (i.e., for this first alternative this would be 248.55) we would like to interpolate. Since we only have two values for a linear-coded price, i.e., the lower and the upper bound, we specify the column index or column names of them. Please be aware that you have to specify the position for each alternative in the validation task, which is why you have to specify the position `c(35, 36)` for each of the six alternatives. Input for `piece.p` is a nested list.

```{r, eval = T}
# load the data
data(acbc)

# define the alternatives
prod1 <- c(3, 6, 10, 13, 16, 20, 24, 32, 248.55)
prod2 <- c(3, 5, 10, 14, 16, 18, 22, 27, 237.39)
prod3 <- c(4, 6, 9, 14, 15, 20, 25, 30, 273.15)
prod4 <- c(4, 5, 10, 11, 16, 19, 26, 32, 213.55)
prod5 <- c(2, 6, 8, 14, 16, 17, 26, 31, 266.10)
prod6 <- c(2, 5, 7, 12, 16, 20, 26, 29, 184.50)

# create validation task
hot_acbc <- create_hot(
  data = acbc,
  id = "id",
  none = "none",
  prod.levels = list(prod1, prod2, prod3, prod4, prod5, prod6),
  coding = c(rep(0, times = 8), 2),
  interpolate.levels = list(c(121.95, 507.95)),
  piece.p = list(
    list(
      c(33, 34), c(33, 34), c(33, 34),
      c(33, 34), c(33, 34), c(33, 34)
    )
  ),
  method = "acbc",
  choice = "hot"
)
```

> If you prefer to treat price as linear or have only one coefficient (i.e., which would be equivalent to just have `price_2` in the data set), please see the following workaround. Use the formula $Value = -1 + 2 * \frac{(price - lower bound)}{(upper bound - lower bound}$:

```{r, eval = TRUE}
prod1 <- c(
  3, 6, 10, 13, 16, 20, 24, 32,
  (-1 + 2 * ((248.55 - 121.95) / (507.95 - 121.95)))
)
prod2 <- c(
  3, 5, 10, 14, 16, 18, 22, 27,
  (-1 + 2 * ((237.39 - 121.95) / (507.95 - 121.95)))
)
prod3 <- c(
  4, 6, 9, 14, 15, 20, 25, 30,
  (-1 + 2 * ((273.15 - 121.95) / (507.95 - 121.95)))
)
prod4 <- c(
  4, 5, 10, 11, 16, 19, 26, 32,
  (-1 + 2 * ((213.55 - 121.95) / (507.95 - 121.95)))
)
prod5 <- c(
  2, 6, 8, 14, 16, 17, 26, 31,
  (-1 + 2 * ((266.10 - 121.95) / (507.95 - 121.95)))
)
prod6 <- c(
  2, 5, 7, 12, 16, 20, 26, 29,
  (-1 + 2 * ((184.50 - 121.95) / (507.95 - 121.95)))
)

hot_acbc <- create_hot(
  data = acbc,
  id = "id",
  none = "none",
  prod.levels = list(prod1, prod2, prod3, prod4, prod5, prod6),
  coding = c(rep(0, times = 8), 1),
  interpolate.levels = list(c(-1, 1)),
  lin.p = "price_2",
  method = "acbc",
  choice = "hot"
)
```

#### Piecewise-coded ACBC

So far, we have only looked at examples where at maximum 1 attribute was not part-worth coded. In the final example, we will have all three types of coding included. We use the data frame `acbc_interpolate` [@sablotny-wackershauser2024] which is also part of the validateHOT package.

The `coding` specified below (`c(0, 1, 0, 0, 0, 0, 0, 0, 2)`) tells us that the second attribute is linear-coded, while the last attribute is piecewise-coded. The other attributes are part-worth coded.

We use the same validation task as before. For `interpolate.levels`, we have to specify two vectors in the list. The first vector contains the values for the linear-coded attribute (`c(9, 10)`), and the second vector contains the breakpoints specified for the piecewise-coded attribute (here price). In `acbc_interpolate`, we provide an example with five breakpoints for the price (`c(121.95, 226.95, 272.95, 326.95, 507.95)`). We specify the column name of the linear-coded attribute in `lin.p` argument. Next, we specify `piece.p` by inserting the column index of the piecewise-coded attribute. The raw utilities for the five breakpoints (`c(121.95, 226.95, 272.95, 326.95, 507.95)`) are stored in the columns `price_1` to `price_5`. Again, we need to specify the lower and upper breakpoints of the value we would like to interpolate in `piece.p`. We must to do this for each of the six alternatives in the validation task. For example, for the first alternative (`248.55`), we see that the lower bound value is `226.95` while the upper bound value is `272.95`. Therefore, we specify the column indexes of these two in `piece.p` (`c(33, 34)`). Alternatively, we could also set it to `c("price_2", "price_3")`. If the price matches a breakpoint, either the lower or upper position can be used.

```{r, eval = T}
# load the data
data(acbc_interpolate)

# define products
prod1 <- c(3, 10, 9, 12, 15, 19, 23, 31, 248.55)
prod2 <- c(3, 9, 9, 13, 15, 17, 21, 26, 237.39)
prod3 <- c(4, 10, 8, 13, 14, 19, 24, 29, 273.15)
prod4 <- c(4, 9, 9, 10, 15, 18, 25, 31, 213.55)
prod5 <- c(2, 10, 7, 13, 15, 16, 25, 30, 266.10)
prod6 <- c(2, 9, 6, 11, 15, 19, 25, 28, 184.50)

# create validation task
hot_acbc_interpolate <- create_hot(
  data = acbc_interpolate,
  id = "id",
  none = "none",
  prod.levels = list(prod1, prod2, prod3, prod4, prod5, prod6),
  coding = c(0, 1, rep(0, times = 6), 2),
  lin.p = "att2",
  interpolate.levels = list(
    c(9, 10),
    c(121.95, 226.95, 272.95, 326.95, 507.95)
  ),
  piece.p = list(
    list(
      c(33, 34), c(33, 34), c(34, 35),
      c(32, 33), c(33, 34), c(32, 33)
    )
  ),
  method = "acbc",
  choice = "hot"
)
```

#### Alternative-Specific Design

If you applied an alternative-specific design in your study and would like to create a validation task with an alternative-specific attribute, please apply `NA` for this attribute level.

## Validation metrics

First, we focus on metrics that are often reported for validation tasks in research papers, namely:

-   Hit Rate (`hitrate()`)
-   Kullback-Leibler-Divergence (`kl()`)
-   Mean Absolute Error (`mae()`)
-   Mean Hit Probability (`mhp()`)
-   Median Absolute Error (`medae()`)
-   Root Mean Square Error (`rmse()`)

All validation metric functions implement the `dplyr` programming logic [@dplyr].

### `hitrate()`

After creating the validation task, the setup is similar for all three methods (i.e., CBC, ACBC, and MaxDiff). For this example, we will use the MaxDiff data set from above. Moreover, since we also would like to show some functions exemplary split by groups, we will use the `varskeep` argument and keep the variable `group` in the new `data.frame` object `hot_mxd`.

```{r}
hot_mxd <- create_hot(
  data = maxdiff,
  id = "id",
  none = "none",
  prod.levels = list(2, 9, 10, 14, 15, 16, 17),
  method = "maxdiff",
  varskeep = "group",
  choice = "hot"
)
```

For the `hitrate()` function, we specify our data (`hot_mxd`), the column names of the alternatives (this includes the no-buy alternative; `opts`), and finally, the actual choice in the validation task (`choice`).

```{r}
hitrate(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice # column name of choice
)
```

### `kl()`

Next, we check the Kullback-Leibler Divergence by running the `kl()` function. Let us first use $log{_2}$ as logarithm base. Alternatively, we could set the logarithm base to `log`.

```{r}
kl(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice, # column name of choice
  log_base = "log2" # logarithm base
)
```

### `mae()`

For the Mean Absolute Error, `mae()`, we split the results by the grouping variable `group`, therefore, we use the `group` argument:

```{r}
mae(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice, # column name of choice
  group = group # Grouping variable
)
```

### `mhp()`

To display the Mean Hit Probability, use the following code chunk:

```{r}
mhp(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice, # column name of choice
  group = group # Grouping variable
)
```

### `medae()`

Next, we look at the Median Absolute Error:

```{r}
medae(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice # column name of choice
)
```

### `rmse()`

And finally, to end this block, we look at the Root Mean Square Error.

```{r}
rmse(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice # column name of choice
)
```

## Confusion matrix

Next, we will take a look at the five metrics of the confusion matrix. The validateHOT package calculates these metrics to determine whether the model correctly predicts purchase behavior or over-/underestimates it. To do so, you need to specify a `none` alternative for the five metrics of confusion matrix, namely ...

-   Accuracy (`accuracy()`)
-   F1-score (`f1()`)
-   Precision (`precision()`)
-   Recall (`recall()`)
-   Specificity (`specificity()`)

> To translate this to the logic of the validateHOT package, imagine a validation task with five alternatives plus the no-buy alternative. validateHOT now measures whether a buy (participant opts for one of the five alternatives) or a no-buy (participant opts for the no-buy alternative), respectively, is correctly predicted.

The setup is similar to the ones of the validation metrics described earlier, the only difference is that you have to specify the column name of the no-buy alternative using the `none` argument.

### `accuracy()`

We begin with the `accuracy()` function and we still work with the `hot_mxd` data frame we created above. We specify the eight alternatives (seven alternatives + no-buy alternative) as well as the `none` argument (i.e.,the column name of the no-buy alternative). The coding is the same for the other four metrics of the confusion matrix: `f1()`, `precision()`, `recall()`, and `specificity()`. Simply replace `accuracy()` with the desired function while keeping the arguments consistent.

```{r}
accuracy(
  data = hot_mxd, # data frame
  opts = c(option_1:none), # column names of alternatives
  choice = choice, # column name of choice
  none = none # column name of none alternative
)
```

## Simulation metrics

Next, let us talk about the simulation metrics. The validateHOT package currently provides the following four functions:

-   Reach of the assortment (`reach()`)
-   Frequency of the assortment (`freqAssort()`)
-   **T**(otal) **U**(nduplicated) **R**(each) and **F**(requency) (`turf()`)
-   TURF ladder
-   Perform a market simulation (`marksim()`)

### `reach()`

Imagine you have a specific assortment and want to check how many participants would consider purchasing at least one of the products or alternatives from it.

For example, let us test three alternatives - `option_1`, `option_4`, and `option_6` - and see how many participants would consider choosing at least one of these alternatives.

> **NOTE**: Both `reach()` and `freqassort()` use the so-called threshold approach, where an alternative is considered as a purchase option if its utility exceeds the specified threshold. In this case, we use the utility of the outside good (i.e., no-buy alternative), though, it could also be the utility of a status quo alternative. A participant is considered "reached", if at least one alternative exceeds this threshold [@chrzan2019].

```{r}
reach(
  # data frame
  data = hot_mxd,
  # alternatives considered in the assortment
  opts = c(option_1, option_4, option_6),
  # threshold that has to be exceeded
  none = none
)
```

### `freqassort()`

The `freqassort()` function calculates how many products or alternatives, on average, are preferred over the no-buy alternative. We will use the same example as we did when introducing the `reach()` function.

```{r}
freqassort(
  # data frame
  data = hot_mxd,
  # alternatives considered in the assortment
  opts = c(option_1, option_4, option_6),
  # threshold that has to be exceeded
  none = none
)
```

### `turf()`

If you are unsure about which bundle of products or alternatives to choose, the `turf()` function helps you determine the optimal combination based on reach and frequency (for explanations, please refer to examples above). When applying turf(), you can decide between applying the *threshold* (`approach = 'thres'`) or *first choice* (`approach = 'fc'`) approach:

-   threshold approach: each alternative that exceeds a threshold is, for example, a purchase option [@chrzan2019, p. 112]

-   first choice approach: only the alternative with the highest utility is considered. If it exceeds the threshold, it is considered, for example, a purchase option [@chrzan2019, p. 111].

Let us assume we ran a MaxDiff study with 16 alternatives and we want to find out which combination of a total of, for example, 3 alternatives can reach most of the participants. Put differently, the combination of 3 alternatives from which most of the participants buy at least one alternative. We use the `maxdiff` data set for this example [@schramm2024].

There are 16 different alternatives included along with an outside option (`none`). To use the `turf()` function, start by specifying the `data.frame` object (`data = "maxdiff"`). Next, define the alternatives (`opts`) that should be considered when creating the different assortments as well as the threshold (`none`) that needs to be exceeded. As described above, the final product assortment should include a total of 3 items (`size = 3`). Finally, we apply the threshold approach (`thres`). We display the first five results using the `head()` function.

```{r}
turf(
  data = maxdiff,
  opts = c(option_01:option_16),
  none = none,
  size = 3,
  approach = "thres"
) %>%
  head(n = 5)
```

By default, the results are sorted in descending order by `reach`. If we decide to rather use the first choice approach, we simply change the `approach` argument to `approach = 'fc'`.

```{r}
maxdiff %>%
  turf(
    opts = c(option_01:option_16),
    none = none,
    size = 3,
    approach = "fc"
  ) %>%
  head(n = 5)
```

We see that the combination with the highest reach has changed. Please be aware that, if `approach = 'fc'`, `reach` and `freq` will be equal since the volume is limited to one per participant.

Let us now imagine the case that we already have two fixed items, meaning that these items have to be included in the assortment, and we want to find two further items that maximize `reach`. In `turf()`, we only have to use the `fixed` argument. Below we assume that `option_02` and `option_12` are fixed. Again, we just display the Top-5 results.

```{r}
turf(
  data = maxdiff,
  opts = c(option_01:option_16),
  none = none,
  size = 4,
  fixed = c("option_02", "option_12"),
  approach = "thres"
) %>%
  head(n = 5)
```

We see that adding `option_13` and `Option_15` yields the highest reach. Again, if you prefer the first choice approach over the threshold approach, just change the `approach` argument.

Finally, you could also specify if there are one or more items or combination of items that should not be included in any of the solutions (i.e., prohibited). For example, let us take the example from above. Both `option_02` and `option_12` should be included. However, neither the combination of `option_02` and `option_04` nor the combination of `option_12` and `option_09` should be included.

```{r}
turf(
  data = maxdiff,
  opts = c(option_01:option_16),
  none = none,
  size = 4,
  fixed = c("option_02", "option_12"),
  prohib = list(c("option_02", "option_04"), c("option_12", "option_09")),
  approach = "thres"
) %>%
  head(n = 5)
```

If you would like to use `turf()` with data collected using, for example, a Likert Scale, some more data wrangling is required. Let's say you have 10 items and for each of the items you asked for the purchase likelihood on a 5-point Likert scale (*1 = "not very likely to purchase"* to *5 = "very likely to purchase"*). You could consider items selected as purchase options if participants rate them as either 4 or 5 on the scale.

Let us create hypothetical data for this example:

```{r}
set.seed(123)
likert <- data.frame(
  alt_01 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_02 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_03 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_04 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_05 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_06 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_07 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_08 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_09 = round(runif(1000, min = 1, max = 5), digits = 0),
  alt_10 = round(runif(1000, min = 1, max = 5), digits = 0)
)
```

To use `turf()`, we need to provide a threshold to the data frame, a value that needs to be exceeded. In our case, we only want to consider option **4** and **5** on the scale, therefore, the threshold needs to be lower than 4 accordingly (e.g., 3.99). In our case, since we only have whole numbers we could also set the threshold to **3**. And afterwards, we run the `turf()` function again.

```{r}
likert %>%
  dplyr::mutate(thres = 3) %>% # adds the threshold variable
  turf(
    opts = c(alt_01:alt_10),
    none = thres,
    size = 3,
    approach = "thres"
  ) %>%
  head(n = 5)
```

### `turf_ladder()`

If we want to run the TURF ladder (i.e., start with one item and then subsequently add one further to maximize reach), we can use the `turf_ladder()` function.

```{r}
turf_ladder(
  data = maxdiff,
  opts = c(option_01:option_16),
  none = none,
  approach = "thres"
)
```

### `marksim()`

Finally, the validateHOT package provides a function, `marksim()`, to calculate the assumed market share of a specified assortment. The package currently provides market simulations based on *share of preference rule* or *first choice rule*. To specify which market simulation method to run, you can easily decide by specifying the `method` argument.

#### Share of Preference

```{r}
hot_mxd %>%
  marksim(
    opts = c(option_1:none), # column names of alternatives
    method = "sop" # market simulation method
  )
```

#### First Choice

```{r}
hot_mxd %>%
  marksim(
    opts = c(option_1:none), # column names of alternatives
    method = "fc" # market simulation method
  )
```

## Converting utilities

Finally, we will show how to convert the raw utilities into scores that are easier for interpretation, namely:

-   `att_imp()` to convert raw utilities of CBC and ACBC into attribute importance scores
-   `prob_scores()` to convert raw utilities of a MaxDiff into choice probabilities
-   `zc_diffs()` to convert raw utilities of CBC and ACBC into zero-centered diffs
-   `zero_anchored()` to convert raw utilities of a (anchored) MaxDiff into zero-centered diffs

We display the aggregated results in the following example (`res = "agg"`), however, if you want to get individual results instead, please specify by setting `res` to `ind`.

### `att_imp()`

Let us again use the examples from above, namely, `cbc`, `cbc_linear`, `acbc`, and `acbc_interpolate`.

#### `cbc`

To use `att_imp()`, we need to specify the `data` argument, the attribute levels for each attribute (`attrib`). Again, all attribute levels need to be provided. Finally, the `coding` argument needs to be specified, which tells `att_imp()` how the attributes were coded (see `coding` above).

```{r}
att_imp(
  data = cbc,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    paste0("att2_lev", c(1:2)),
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    paste0("price_", c(1:6))
  ),
  coding = c(rep(0, times = 9)),
  res = "agg"
)
```

#### `cbc_linear`

Next, we assume that one of the attributes was linear-coded (`price`). Again, as we had to do for `create_hot()`, we need to provide the values that we used for model estimation (see above). We again specify those in the argument `interpolate.levels`.

```{r}
att_imp(
  data = cbc_linear,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    paste0("att2_lev", c(1:2)),
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    "price"
  ),
  interpolate.levels = list(c(seq(from = 175.99, to = 350.99, by = 35))),
  coding = c(rep(0, times = 8), 1),
  res = "agg"
)
```

#### `acbc`

Above, we explained how to address a piecewise-coded ACBC in `create_hot()`. We again specify a linear-coded price in ACBC as piecewise-coded (see explanation above).

```{r}
att_imp(
  data = acbc,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    paste0("att2_lev", c(1:2)),
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    paste0("price_", c(1:2))
  ),
  coding = c(rep(0, times = 8), 2),
  res = "agg"
)
```

#### `acbc_interpolate`

Finally, in `acbc_interpolate` we also had one linear-coded attribute, which we specify in the coding again.

```{r}
att_imp(
  data = acbc_interpolate,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    "att2",
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    paste0("price_", c(1:5))
  ),
  interpolate.levels = list(c(9, 10)),
  coding = c(0, 1, rep(0, times = 6), 2),
  res = "agg"
)
```


### `zc_diffs()`

The input is almost identical to `att_imp()`, the only difference in case you included a `none` option in your study, you need to specify it here by defining the `none` argument.

#### `cbc`

```{r}
zc_diffs(
  data = cbc,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    paste0("att2_lev", c(1:2)),
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    paste0("price_", c(1:6))
  ),
  coding = c(rep(0, times = 9)),
  res = "agg",
  none = "none"
)
```

#### `cbc_linear`

```{r}
zc_diffs(
  data = cbc_linear,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    paste0("att2_lev", c(1:2)),
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    "price"
  ),
  interpolate.levels = list(c(seq(from = 175.99, to = 350.99, by = 35))),
  coding = c(rep(0, times = 8), 1),
  res = "agg",
  none = "none"
)
```

#### `acbc`

```{r}
zc_diffs(
  data = acbc,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    paste0("att2_lev", c(1:2)),
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    paste0("price_", c(1:2))
  ),
  coding = c(rep(0, times = 8), 2),
  res = "agg",
  none = "none"
)
```

#### `acbc_interpolate`

```{r}
zc_diffs(
  data = acbc_interpolate,
  attrib = list(
    paste0("att1_lev", c(1:3)),
    "att2",
    paste0("att3_lev", c(1:4)),
    paste0("att4_lev", c(1:4)),
    paste0("att5_lev", c(1:2)),
    paste0("att6_lev", c(1:4)),
    paste0("att7_lev", c(1:6)),
    paste0("att8_lev", c(1:6)),
    paste0("price_", c(1:5))
  ),
  interpolate.levels = list(c(9, 10)),
  coding = c(0, 1, rep(0, times = 6), 2),
  res = "agg",
  none = "none"
)
```

### `prob_scores()`

`prob_scores()` calculates the choice probability scores of a (anchored) MaxDiff [@chrzan2019]. Let us first imagine we ran a normal MaxDiff (no anchoring). After specifying the `data.frame` object (`data`) as well as the `items`, we have to specify the argument `set.size` which defines the number of items that were shown per MaxDiff task. In this example, a participant saw `4` items per task from which s/he had to choose the best and worst item.

```{r}
prob_scores(
  data = maxdiff,
  items = c(option_01:option_16),
  set.size = 4,
  res = "agg"
)
```

The code for the anchored MaxDiff is the same, only that we also have to specify the `anchor` argument, which is the the `none` alternative, however, it can also be another threshold variable.

```{r}
prob_scores(
  data = maxdiff,
  items = c(option_01:none),
  set.size = 4,
  res = "agg",
  anchor = none
)
```

### `zero_anchored()`

`zero_anchored()` converts raw utilities of a (anchored) MaxDiff into zero-centered diffs [@chrzan2019, p. 64].

The analysis is almost identical to `prob_scores()` introduced above.

```{r}
zero_anchored(
  data = maxdiff,
  items = c(option_01:option_16),
  res = "agg"
)
```

The code for the anchored MaxDiff is the same, but we also have to specify the `anchor` argument, which is the the `none` alternative, however, can also be another threshold variable.

```{r}
zero_anchored(
  data = maxdiff,
  items = c(option_01:none),
  res = "agg",
  anchor = none
)
```

## References
