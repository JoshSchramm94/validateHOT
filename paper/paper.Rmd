---
title: 'validateHOT - Validate your Holdout Task'
tags:
  - R
  - MaxDiff
  - Conjoint Analysis
  - Market Simulations
  - Predictive Validity
authors:
  - name: Joshua Schramm
    orcid: 0000-0001-5602-4632
    corresponding: True
    affiliation: 1
  - name: Marcel Lichters
    orcid: 0000-0002-3710-2292
    corresponding: FALSE
    affiliation: 1, 2    
affiliations:
 - name: Chemnitz University of Technology, Germany
   index: 1
 - name: Otto von Guericke University of Magdeburg, Germany
   index: 2
citation_author: Schramm & Lichters
date: 22 October 2023
year: 2023
bibliography: paper.bib
link-citations: true
output: rticles::joss_article
journal: JOSS
---

```{=tex}
\setlength{\headheight}{63.55022pt}
\addtolength{\topmargin}{-0.95425pt}
\newcommand{\colcod}[1]{\texttt{\color{purple}#1}}
```
```{r include=FALSE}
library(validateHOT)
library(tidyverse)
```

# Summary

validateHOT is a package that provides functions to both validate a validation/holdout task and run market simulations for results obtained in a (adaptive) choice-based conjoint analysis (hereafter ACBC and CBC, respectively) and maximum difference scaling (hereafter MaxDiff) using [Sawtooth Software](https://sawtoothsoftware.com/).[^1]

[^1]: validateHOT also works with MaxDiff raw logit utilities, see, for example, @chapman2023. For CBC, it should also work with other sofware, as long as all attributes are part-worth coded. Linear and piecewise coding is implemented to work with Sawtooth Software.

Preference measurement techniques', such as (A)CBC or MaxDiff, ultimate goal is to predict future behavior [@green1990]. Hence, it is essential for both academics and practitioners to ensure that the collected data is valid and can also predict outside tasks (i.e., the model has external validity). In terms of external validity, we refer to the generalizations to different settings [see, @calder1982, p.240]. The easiest way to test this is to include so-called validation or holdout tasks [@Orme2015], which are tasks that are fixed (i.e., same across participant) and are usually not used for estimating the part-worth utilities in hierarchical Bayes estimation. Despite the important role of validation tasks, practitioners often do not include them [@yang2018]. Given the fact that the model is used to estimate market shares which poses the basis for relevant marketing decisions, the current approach is unsatisfactory.

validateHOT combines both validation and market simulation in one package and has three key advantages: a) it helps you to decide which is the best model to proceed by validating it, b) it runs relevant market simulations that help to find the right product combinations, and finally, c) it is an open source tool including functions that are usually implemented in paid software, and therefore, remain a black-box for researchers and practitioners.

# Statement of need

validateHOT is a practical tool for Sawtooth Software users in industry as well as academia. It provides an open source solution for a) validating a validation/holdout task and therefore ensuring that the model has predictive validity; b) running market simulations (e.g., **T**otal **U**nduplicated **R**each and **F**requency, hereafter TURF). Other packages, for example, Metrics [@Metrics] provide functions to run validation metrics such as *mean absolute error*, *root mean squared error*, or the five metrics of the confusion matrix (see \autoref{tab:table1}). However, to put the Sawtooth export into the right format, the user needs some data wrangling which could pose a barrier. Moreover, packages mainly focus on the analysis of conjoint analysis (e.g., ChoiceModelR [@ChoiceModelR], choicetools [@choicetools], logitR [@logitr], bayesm [@bayesm] etc.). To the best of our knowledge, a package that converts raw utility scores into validation metrics or running a variety of marketing simulations (especially TURF) is still missing. validateHOT is especially helpful for researchers who plan to report results for research articles according to open science standards. In addition, since the former package turfR is no longer available on the Comprehensive R Archive Network (see [CRAN](https://cran.r-project.org/web/packages/turfR/index.html)), academics and practitioners do not have an alternative function in *R* to run this method. Therefore, practitioners and academics mainly have to stick to paid solutions.

# Key functions

validateHOT's functions can be categorized into four main components, see \autoref{tab:table1}. To bring the data into the right format, users can run the \texttt{\color{purple}createHOT} function, which creates the total utility of each alternative by applying the additive utility model [@rao2014, p. 82]. \colcod{turf} as well as the 2 rescaling functions, however, are not dependent on \texttt{\color{purple}createHOT}, and can be run using the raw logit scores.

| Validation metrics | Confusion matrix | Market simulations | Rescaling scores |
|:----------------:|:----------------:|:----------------:|:----------------:|
|     hitrate()      |    accuracy()    |    freqassort()    |    att_imp()     |
|        kl()        |       f1()       |     marksim()      |  prob_scores()   |
|       mae()        |   precision()    |      reach()       |                  |
|      medae()       |     recall()     |       turf()       |                  |
|       mhp()        |  specificity()   |                    |                  |
|       rmse()       |                  |                    |                  |

: Overview of main four components of validateHOT and their corresponding functions \label{tab:table1}

# Typical workflow

In the following, we provide the workflow for a MaxDiff study (the vignette also provides detailed examples for a CBC as well as an ACBC).

After running the Hierarchical Bayes estimation in, for example, Sawtooth Software, the **raw** utility scores have to be exported and afterwards read into *R* as a data frame. This data frame also has to include the actual choice in the validation/holdout task.

To define the validation/holdout task, which has a total of 7 items (\texttt{\color{purple}prod}) plus the no-buy alternative (\texttt{\color{purple}none}), we use the \texttt{\color{purple}createHOT} function. Here, the user can define the attributes as well as the method (here \texttt{\color{purple}MaxDiff}).

```{r}
data("MaxDiff") # read in the data
HOT <- createHOT(
  data = MaxDiff, # data frame
  id = 1, # index unique identifier
  none = 19, # index of none alternative
  prod = 7, # no of alternatives in HOT excluding none
  prod.levels = list(3, 10, 11, 15, 16, 17, 18), # index of alternatives
  method = "MaxDiff", # method applied
  choice = 20, # column index of choice alternative
  varskeep = 21
)
```

Next, to get the relevant validation metrics that are often reported in conjoint studies, for example, hit rate [e.g., @ding2005], mean hit probability [mhp, @voleti2017], or mean absolute error [mae, @wloemert2014], we only need to provide the data, the alternatives in the validation/holdout task (\texttt{\color{purple}opts}), and the actual choice (\texttt{\color{purple}choice}). Everything can be implemented in the tidyverse [@tidyverse] logic.

```{r}
hitrate(
  data = HOT, # data frame
  opts = c(Option_1:None), # column names of alternatives
  choice = choice # column name of choice
) %>%
  round(3)
```

validateHOT also provides the five metrics for the confusion matrix. The underlying logic hereby is that the user has to provide a no-buy alternative (\texttt{\color{purple}none}). validateHOT calculates, for example, how often a buy or no-buy was correctly predicted, therefore, it is testing whether the model correctly predicts general demand (exemplary showed by applying the \texttt{\color{purple}accuracy} function and results split by \texttt{\color{purple}Group}).

```{r}
accuracy(
  data = HOT, # data frame
  group = Group, # optional grouping variable
  opts = c(Option_1:None), # column names of alternatives
  choice = choice, # column name of choice
  none = None # column name
)
```

Finally, we show two functions for market simulations, namely \texttt{\color{purple}marksim} as well as \texttt{\color{purple}turf}. First, we calculate the market shares based on the multinomial logit model [@McFadden1974]. Besides the aggregated shares, \texttt{\color{purple}marksim} also provides standard errors and the 95% confidence interval.

```{r}
marksim(
  data = HOT,
  opts = c(Option_1:None),
  method = "sop"
)
```

\texttt{\color{purple}turf}, a "product line extension model" [@miaoulis1990, p. 29], is a tool to find the perfect assortment that creates the highest reach and especially powerful for MaxDiff studies [@chrzan2019, p. 108]. To optimize the search for the optimal bundle, we also include the arguments \texttt{\color{purple}fixed}, to define alternatives that have to be part of the assortment, as well as \texttt{\color{purple}prohib}, to define prohibitions of combinations of items that should not be part of the assortment (please see the vignette for more details as well as how to apply \texttt{\color{purple}turf} with data obtained using a likert scale).

For the following example, let us assume that the user conducted an anchored MaxDiff analysis with 10 items (\texttt{\color{purple}opts}) and now wants to find the best assortment with a size of 3 (\texttt{\color{purple}size = 3}). As a threshold (\texttt{\color{purple}none}), the user uses the anchor (no-buy alternative).

```{r}
turf(
  data = MaxDiff, # define data
  opts = c(Option_01:Option_10), # define items
  none = none, # define threshold variable
  size = 3, # define size of assortment
  approach = "thres" # define approach
) %>%
  head(., n = 5) %>%
  mutate_if(is.numeric, round, 2) %>%
  t() %>%
  as.data.frame() %>%
  slice(-1) %>%
  rename_all(., ~ paste0("Combo ", c(1:5)))
```

# Availability

validateHOT is available on [Github](https://github.com/JoshSchramm94/validateHOT).

# References
