---
output: github_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: 72
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# validateHOT üéØ

<!-- badges: start -->

<!-- badges: end -->

The goal of validateHOT is to validate the results of your validation
task (also known as holdout task). A validation task is essential to
make sure that your collected data of a MaxDiff, CBC, or ACBC are
valid and can also predict outside task that were not included in
estimating your utility scores. Although commercial studies often do not
include a validation/holdout task [@yang2018], it is highly recommended
to do so [@Orme.2015; @rao2014a]. This validation/ holdout task does not
only help to check whether everything went right during data collection
but also to determine your final model. <code>validatHOT</code> provides
some of the relevant metrics to test the performance of your data in
predicting a holdout task. In [Sawtooth
Software's](https://sawtoothsoftware.com/) CBC a fixed validation/
holdout task is automatically implemented. If you conduct a MaxDiff or
ACBC study these have to be programmed by yourself.

üëâüèæ <u>**What you need to provide**</u>: <br> After collecting your data,
and running your initial Hierarchical Bayes models, you can turn to
<code>validateHOT</code> and test how good your model predicts choices
in the validation/ holdout task. Herefore, you only have to insert your
raw utility scores as well as the actual choice of your validation/
holdout task. You can use the <code>merge()</code> provided by
<code>base</code> package [@base]. Afterward, you can read in your data
file and enjoy <code>validateHOT</code>.

üëàüèæ <u>**What you get**</u>:<br> At the moment, <code>validateHOT</code>
provides functions for 3 key areas:

<ul>

<li>validation metrics mainly reported in preference measurement
studies</li>

<li>metrics that are usually reported in machine learning (i.e.,
confusion matrix)</li>

<li>simulation methods, for example, to determine optimal product
combinations</li>

</ul>

For all 3 key areas, the <code>createHOT</code> function is essential.
This function creates the total utilities for each alternative in the
validation/holdout task and also in the simulation method, respectively.
@rao2014a [p. 82] mentions the additive utility model stating that the
total utility of a profile or conjoint model is the sum of its attribute
levels. <code>createHOT</code> will do exactly this for you.

### Classical validation metrics

<ul>

<li><code>hitrate</code>: creates the Hit Rate (correctly predicted
choices) of your validation task. The output will contain the chance
level in your validation task ($\frac{1}{alternatives}$) in percentage.
The number of correctly predicted participants' choices as well as the
percentage of how many choices were correctly predicted. If you specify
an optional <code>Group</code> argument the output is split by
groups.</li>

<li><code>kl</code>: Kullback-Leibler-Divergence which measures the
divergence between the actual choice distribution and the predicted
choice distribution [@ding2011; @philentropy]. Output provides both
divergence between predicted from observed and observed from
predicted due to the asymmetry of the Kullback-Leibler divergence. If
you specify an optional <code>Group</code> argument the output is split
by groups.</li>

<li><code>mae</code>: average absolute error, i.e., deviation between
predicted and stated choice share. If you specify an optional
<code>Group</code> argument the output is split by groups.</li>

<li><code>medae</code>: since the averaged absolute error can be highly
influenced by the If you specify an optional <code>Group</code> argument
the output is split by groups.</li>

<li><code>mhp</code>: averaged hit probability of participant's actual
choice in the validation/ holdout task. If you specify an optional
<code>Group</code> argument the output is split by groups.</li>

<li><code>rmse</code>: provides the root-mean-squared error of deviation
between predicted and stated choice share. If you specify an optional
<code>Group</code> argument the output is split by groups.</li>

</ul>

### Confusion Matrix

We also include metrics from machine learning, i.e., the confusion matrix (e.g., @Burger2018). For all of the 5 provided functions, you currently have to have a **none** option in your data. We currently predict, e.g., whether a buy or no-buy was correctly predicted. Information could be used for overestimating and underestimating, respectively, of product purchases.

<ul>

<li><code>accuracy</code>: $\frac{2 * precision * recall}{precision + recall}$</li>

<li><code>f1</code>: </li>

<li><code>precision</code>: </li>

<li><code>recall</code>: </li>

<li><code>specificity</code>: </li>

</ul>

### Simulation Methods

<ul>

<li><code>freqassort</code>: Inspired by the former
[turfR](https://github.com/cran/turfR) package,
<code>freqassort</code> will give you the averaged frequency, how many
products the participants will choose from your in the function
determined potential assortment. For the <code>method</code> argument
you can decide between <code>method = "threshold"</code> (if utility of
product is larger than the utility of <code>None</code>, it is marked as potential
purchase option) and <code>method = "First Choice"</code> (only product
with highest utility is considered. If its utility is above the utility
of <code>None</code>, it is marked as potential purchase option). If you specify an
optional <code>Group</code> argument the output is split by groups.</li>

<li><code>reach</code>: Inspired by the former
[turfR](https://github.com/cran/turfR) package, <code>reach</code>
will give you the averaged percentage of how many participants you can
reach (buy at least one of the products) with your in the function
determined potential assortment. or the <code>method</code> argument you
can decide between <code>method = "threshold"</code> (if utility of
product is larger than the utility of <code>None</code>, it is marked as potential
purchase option) and <code>method = "First Choice"</code> (only product
with highest utility is considered. If its utility is above the utility
of <code>None</code>, it is marked as potential purchase option). If you specify an
optional <code>Group</code> argument the output is split by groups.</li>

<li><code>shareofpref</code>: provides you the aggregated share of
preference, including the lower and upper confidence interval, which is
calculated according to the $mean +/- 1.96 x \frac{sd}{\sqrt(n)}$. If
you specify an optional <code>Group</code> argument the output is split
by groups and provided in a list element.</li>

</ul>

### Data Frames provided by <code>validateHOT</code>

<ul>

<li><code>ACBC</code>: Example data set with raw utilities of an ACBC
study conducted in Sawtooth. Price was linear-coded while the other
attributes were coded as part-worths.</li>

<li><code>ACBC_interpolate</code>: Example data set with raw utilities
of an ACBC study conducted in Sawtooth. Price was piecewise-coded,
another attribute was linear-coded while the other attributes were coded
as part-worths.</li>

<li><code>CBC</code>: Example data set with raw utilities of an CBC
study conducted in Sawtooth. All attributes were coded as
part-worth.</li>

<li><code>CBC_lin</code>: Example data set with raw utilities of an
CBC study conducted in Sawtooth. One attribute was linear coded while
the other attributes are part-worth coded.</li>

<li><code>MaxDiff</code>: Example data set with raw utilities of an
MaxDiff study conducted in Sawtooth.</li>

</ul>

<ul>

<li><code>accuracy</code>: generates the number of correct predicted
choice or no-choice divided by the total number of predictions. Only
possible for a binary output, e.g., buy vs. no-buy correctly
predicted.</li>

<li><code>f1</code>: generates the F1-Score. F1-Score is calculated by
the following formula
$\frac{2 * precision * recall}{precision + recall}$. Only possible for a
binary output, e.g., buy vs. no-buy correctly predicted.</li>

</ul>

## Why <code>validateHOT</code>

comparison to Metrics

studi seminar f√ºr preference measurement techniques --\> often not
getting in touch with *R* before, so wanted to make it easier for them.
this

## Installation

You can install the development version of validateHOT from
[GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("JoshSchramm94/validateHOT")
```

## Example



## References

